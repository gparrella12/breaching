Trial 0

Investigating use case small_batch_imagenet with server type honest_but_curious.
Seed: 79469
Model architecture vggface2 loaded with 27,910,327 parameters and 29,712 buffers.
Overall this is a data ratio of      23:1 for target shape [8, 3, 224, 224] given that num_queries=1.
User (of type UserSingleStep) with settings:
    Number of data points: 8

    Threat model:
    User provides labels: False
    User provides buffers: True
    User provides number of data points: True

    Data:
    Dataset: flickr_faces
    user: 0
    
        
Server (of type HonestServer) with settings:
    Threat model: Honest-but-curious
    Number of planned queries: 1
    Has external/public data: False

    Model:
        model specification: vggface2
        model state: default
        public buffers: False

    Secrets: {}
    
Attacker (of type OptimizationBasedAttacker) with settings:
    Hyperparameter Template: see-through-gradients

    Objective: Euclidean loss with scale=0.0001 and task reg=0.0
    Regularizers: Total Variation, scale=0.0001. p=1 q=1. 
                  Input L^p norm regularization, scale=1e-06, p=2
                  Deep Inversion Regularization (matching batch norms), scale=0.1, first-bn-mult=10
    Augmentations: 

    Optimization Setup:
        optimizer: adam
        signed: False
        step_size: 0.1
        boxed: True
        max_iterations: 80000
        step_size_decay: cosine-decay
        langevin_noise: 0.01
        warmup: 100
        grad_clip: None
        callback: 1000
        
Computing user update on user 0 in model mode: training.
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_pre0.png
Reconstructing user data...
Files already downloaded and verified
initial data len: 8
Recovered labels [177, 180, 387, 498, 807, 920, 958, 998] through strategy yin.
| It: 1 | Rec. loss: 29.8580 |  Task loss: 12.3154 | T: 1.32s
| It: 1001 | Rec. loss: 9.1925 |  Task loss: 0.1820 | T: 220.36s
| It: 2001 | Rec. loss: 8.9730 |  Task loss: 0.1629 | T: 207.30s
| It: 3001 | Rec. loss: 8.8790 |  Task loss: 0.1557 | T: 210.43s
| It: 4001 | Rec. loss: 8.8582 |  Task loss: 0.1424 | T: 208.00s
| It: 5001 | Rec. loss: 8.7539 |  Task loss: 0.1529 | T: 205.92s
| It: 6001 | Rec. loss: 8.7121 |  Task loss: 0.1619 | T: 214.51s
| It: 7001 | Rec. loss: 8.6617 |  Task loss: 0.1538 | T: 205.62s
| It: 8001 | Rec. loss: 8.6216 |  Task loss: 0.1438 | T: 204.08s
| It: 9001 | Rec. loss: 8.5757 |  Task loss: 0.1412 | T: 202.30s
| It: 10001 | Rec. loss: 8.4979 |  Task loss: 0.1395 | T: 210.40s
| It: 11001 | Rec. loss: 8.4908 |  Task loss: 0.1428 | T: 208.27s
| It: 12001 | Rec. loss: 8.4148 |  Task loss: 0.1516 | T: 208.77s
| It: 13001 | Rec. loss: 8.3545 |  Task loss: 0.1388 | T: 212.65s
| It: 14001 | Rec. loss: 8.2843 |  Task loss: 0.1438 | T: 209.00s
| It: 15001 | Rec. loss: 8.2405 |  Task loss: 0.1402 | T: 201.97s
| It: 16001 | Rec. loss: 8.1372 |  Task loss: 0.1376 | T: 201.67s
| It: 17001 | Rec. loss: 8.0951 |  Task loss: 0.1374 | T: 208.27s
| It: 18001 | Rec. loss: 8.0027 |  Task loss: 0.1390 | T: 201.51s
| It: 19001 | Rec. loss: 7.8936 |  Task loss: 0.1295 | T: 205.43s
| It: 20001 | Rec. loss: 7.8393 |  Task loss: 0.1285 | T: 206.28s
| It: 21001 | Rec. loss: 7.7335 |  Task loss: 0.1378 | T: 202.21s
| It: 22001 | Rec. loss: 7.6351 |  Task loss: 0.1248 | T: 201.25s
| It: 23001 | Rec. loss: 7.4874 |  Task loss: 0.1278 | T: 203.59s
| It: 24001 | Rec. loss: 7.3587 |  Task loss: 0.1354 | T: 208.73s
| It: 25001 | Rec. loss: 7.2469 |  Task loss: 0.1235 | T: 211.27s
| It: 26001 | Rec. loss: 7.1502 |  Task loss: 0.1307 | T: 202.93s
| It: 27001 | Rec. loss: 6.9834 |  Task loss: 0.1229 | T: 203.93s
| It: 28001 | Rec. loss: 6.8453 |  Task loss: 0.1220 | T: 208.74s
| It: 29001 | Rec. loss: 6.6883 |  Task loss: 0.1247 | T: 209.39s
| It: 30001 | Rec. loss: 6.5091 |  Task loss: 0.1170 | T: 201.26s
| It: 31001 | Rec. loss: 6.3859 |  Task loss: 0.1107 | T: 210.61s
| It: 32001 | Rec. loss: 6.2208 |  Task loss: 0.1138 | T: 212.27s
| It: 33001 | Rec. loss: 6.0455 |  Task loss: 0.1123 | T: 201.81s
| It: 34001 | Rec. loss: 5.8701 |  Task loss: 0.1110 | T: 203.22s
| It: 35001 | Rec. loss: 5.6943 |  Task loss: 0.1115 | T: 202.69s
| It: 36001 | Rec. loss: 5.4775 |  Task loss: 0.1060 | T: 204.50s
| It: 37001 | Rec. loss: 5.2870 |  Task loss: 0.1081 | T: 214.42s
| It: 38001 | Rec. loss: 5.0927 |  Task loss: 0.1084 | T: 237.19s
| It: 39001 | Rec. loss: 4.8871 |  Task loss: 0.1070 | T: 230.16s
| It: 40001 | Rec. loss: 4.6867 |  Task loss: 0.1045 | T: 228.55s
| It: 41001 | Rec. loss: 4.4807 |  Task loss: 0.1058 | T: 232.92s
| It: 42001 | Rec. loss: 4.2875 |  Task loss: 0.1043 | T: 230.87s
| It: 43001 | Rec. loss: 4.0883 |  Task loss: 0.1031 | T: 223.69s
| It: 44001 | Rec. loss: 3.9029 |  Task loss: 0.0967 | T: 231.35s
| It: 45001 | Rec. loss: 3.7260 |  Task loss: 0.1045 | T: 234.20s
| It: 46001 | Rec. loss: 3.5578 |  Task loss: 0.0989 | T: 224.98s
| It: 47001 | Rec. loss: 3.3845 |  Task loss: 0.0961 | T: 219.85s
| It: 48001 | Rec. loss: 3.2215 |  Task loss: 0.0958 | T: 226.07s
| It: 49001 | Rec. loss: 3.0620 |  Task loss: 0.0948 | T: 220.73s
| It: 50001 | Rec. loss: 2.9145 |  Task loss: 0.0890 | T: 223.02s
| It: 51001 | Rec. loss: 2.7729 |  Task loss: 0.0920 | T: 221.21s
| It: 52001 | Rec. loss: 2.6340 |  Task loss: 0.0869 | T: 225.87s
| It: 53001 | Rec. loss: 2.5091 |  Task loss: 0.0863 | T: 223.73s
| It: 54001 | Rec. loss: 2.3818 |  Task loss: 0.0859 | T: 224.27s
| It: 55001 | Rec. loss: 2.2599 |  Task loss: 0.0856 | T: 226.16s
| It: 56001 | Rec. loss: 2.1503 |  Task loss: 0.0848 | T: 232.92s
| It: 57001 | Rec. loss: 2.0467 |  Task loss: 0.0808 | T: 224.03s
| It: 58001 | Rec. loss: 1.9466 |  Task loss: 0.0817 | T: 225.99s
| It: 59001 | Rec. loss: 1.8551 |  Task loss: 0.0806 | T: 232.97s
| It: 60001 | Rec. loss: 1.7748 |  Task loss: 0.0784 | T: 221.22s
| It: 61001 | Rec. loss: 1.6958 |  Task loss: 0.0795 | T: 205.12s
| It: 62001 | Rec. loss: 1.6258 |  Task loss: 0.0798 | T: 209.89s
| It: 63001 | Rec. loss: 1.5768 |  Task loss: 0.0781 | T: 224.22s
| It: 64001 | Rec. loss: 1.5018 |  Task loss: 0.0782 | T: 230.45s
| It: 65001 | Rec. loss: 1.4504 |  Task loss: 0.0770 | T: 227.18s
| It: 66001 | Rec. loss: 1.4055 |  Task loss: 0.0769 | T: 229.26s
| It: 67001 | Rec. loss: 1.3581 |  Task loss: 0.0761 | T: 226.46s
| It: 68001 | Rec. loss: 1.3157 |  Task loss: 0.0765 | T: 223.43s
| It: 69001 | Rec. loss: 1.2839 |  Task loss: 0.0754 | T: 233.75s
| It: 70001 | Rec. loss: 1.2525 |  Task loss: 0.0748 | T: 226.95s
| It: 71001 | Rec. loss: 1.2264 |  Task loss: 0.0750 | T: 233.60s
| It: 72001 | Rec. loss: 1.1998 |  Task loss: 0.0748 | T: 230.03s
| It: 73001 | Rec. loss: 1.1790 |  Task loss: 0.0744 | T: 231.53s
| It: 74001 | Rec. loss: 1.1624 |  Task loss: 0.0741 | T: 230.06s
| It: 75001 | Rec. loss: 1.1479 |  Task loss: 0.0739 | T: 233.02s
| It: 76001 | Rec. loss: 1.1347 |  Task loss: 0.0741 | T: 233.23s
| It: 77001 | Rec. loss: 1.1251 |  Task loss: 0.0741 | T: 249.65s
| It: 78001 | Rec. loss: 1.1184 |  Task loss: 0.0741 | T: 243.75s
| It: 79001 | Rec. loss: 1.1146 |  Task loss: 0.0742 | T: 243.00s
| It: 80000 | Rec. loss: 1.1136 |  Task loss: 0.0742 | T: 243.01s
Optimal candidate solution with rec. loss 2013.1210 selected.
Reconstruction stats:
Starting evaluations for attack effectiveness report...
The size of tensor a (55) must match the size of tensor b (61) at non-singleton dimension 3
None
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_post0.png
========================================================================

Trial 1

Investigating use case small_batch_imagenet with server type honest_but_curious.
Seed: 82107
Model architecture vggface2 loaded with 27,910,327 parameters and 29,712 buffers.
Overall this is a data ratio of      23:1 for target shape [8, 3, 224, 224] given that num_queries=1.
User (of type UserSingleStep) with settings:
    Number of data points: 8

    Threat model:
    User provides labels: False
    User provides buffers: True
    User provides number of data points: True

    Data:
    Dataset: flickr_faces
    user: 0
    
        
Server (of type HonestServer) with settings:
    Threat model: Honest-but-curious
    Number of planned queries: 1
    Has external/public data: False

    Model:
        model specification: vggface2
        model state: default
        public buffers: False

    Secrets: {}
    
Attacker (of type OptimizationBasedAttacker) with settings:
    Hyperparameter Template: see-through-gradients

    Objective: Euclidean loss with scale=0.0001 and task reg=0.0
    Regularizers: Total Variation, scale=0.0001. p=1 q=1. 
                  Input L^p norm regularization, scale=1e-06, p=2
                  Deep Inversion Regularization (matching batch norms), scale=0.1, first-bn-mult=10
    Augmentations: 

    Optimization Setup:
        optimizer: adam
        signed: False
        step_size: 0.1
        boxed: True
        max_iterations: 80000
        step_size_decay: cosine-decay
        langevin_noise: 0.01
        warmup: 100
        grad_clip: None
        callback: 1000
        
Computing user update on user 0 in model mode: training.
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_pre1.png
Reconstructing user data...
Files already downloaded and verified
initial data len: 8
Recovered labels [58, 243, 350, 475, 476, 705, 834, 857] through strategy yin.
| It: 1 | Rec. loss: 27.9643 |  Task loss: 10.3645 | T: 0.28s
| It: 1001 | Rec. loss: 8.8273 |  Task loss: 0.1463 | T: 254.69s
| It: 2001 | Rec. loss: 8.7277 |  Task loss: 0.1413 | T: 244.55s
| It: 3001 | Rec. loss: 8.6202 |  Task loss: 0.1400 | T: 242.06s
| It: 4001 | Rec. loss: 8.5802 |  Task loss: 0.1296 | T: 248.22s
| It: 5001 | Rec. loss: 8.5471 |  Task loss: 0.1337 | T: 239.07s
| It: 6001 | Rec. loss: 8.4877 |  Task loss: 0.1315 | T: 232.27s
| It: 7001 | Rec. loss: 8.4565 |  Task loss: 0.1410 | T: 226.99s
| It: 8001 | Rec. loss: 8.4550 |  Task loss: 0.1373 | T: 239.79s
| It: 9001 | Rec. loss: 8.3857 |  Task loss: 0.1298 | T: 235.30s
| It: 10001 | Rec. loss: 8.3544 |  Task loss: 0.1232 | T: 248.56s
| It: 11001 | Rec. loss: 8.2734 |  Task loss: 0.1257 | T: 245.51s
| It: 12001 | Rec. loss: 8.2204 |  Task loss: 0.1149 | T: 238.93s
| It: 13001 | Rec. loss: 8.1580 |  Task loss: 0.1155 | T: 241.35s
| It: 14001 | Rec. loss: 8.0861 |  Task loss: 0.1162 | T: 237.26s
| It: 15001 | Rec. loss: 8.0137 |  Task loss: 0.1176 | T: 238.04s
| It: 16001 | Rec. loss: 7.9276 |  Task loss: 0.1230 | T: 238.09s
| It: 17001 | Rec. loss: 7.8623 |  Task loss: 0.1105 | T: 238.78s
| It: 18001 | Rec. loss: 7.7637 |  Task loss: 0.1117 | T: 240.90s
| It: 19001 | Rec. loss: 7.7045 |  Task loss: 0.1187 | T: 245.02s
| It: 20001 | Rec. loss: 7.5664 |  Task loss: 0.1200 | T: 236.83s
| It: 21001 | Rec. loss: 7.4659 |  Task loss: 0.1083 | T: 248.04s
| It: 22001 | Rec. loss: 7.3506 |  Task loss: 0.1114 | T: 238.49s
| It: 23001 | Rec. loss: 7.2354 |  Task loss: 0.1160 | T: 242.03s
| It: 24001 | Rec. loss: 7.1176 |  Task loss: 0.1119 | T: 239.67s
| It: 25001 | Rec. loss: 7.0197 |  Task loss: 0.1056 | T: 240.15s
| It: 26001 | Rec. loss: 6.8639 |  Task loss: 0.1106 | T: 243.87s
| It: 27001 | Rec. loss: 6.7545 |  Task loss: 0.1071 | T: 234.19s
| It: 28001 | Rec. loss: 6.5986 |  Task loss: 0.1094 | T: 241.61s
| It: 29001 | Rec. loss: 6.4567 |  Task loss: 0.1031 | T: 236.83s
| It: 30001 | Rec. loss: 6.3354 |  Task loss: 0.0995 | T: 238.26s
| It: 31001 | Rec. loss: 6.2099 |  Task loss: 0.1065 | T: 234.47s
| It: 32001 | Rec. loss: 6.0146 |  Task loss: 0.1035 | T: 241.74s
| It: 33001 | Rec. loss: 5.8679 |  Task loss: 0.0980 | T: 236.64s
| It: 34001 | Rec. loss: 5.6862 |  Task loss: 0.0950 | T: 236.19s
| It: 35001 | Rec. loss: 5.5510 |  Task loss: 0.0972 | T: 239.01s
| It: 36001 | Rec. loss: 5.3566 |  Task loss: 0.0980 | T: 243.03s
| It: 37001 | Rec. loss: 5.1839 |  Task loss: 0.0932 | T: 241.87s
| It: 38001 | Rec. loss: 5.0012 |  Task loss: 0.0959 | T: 245.82s
| It: 39001 | Rec. loss: 4.8208 |  Task loss: 0.0954 | T: 246.97s
| It: 40001 | Rec. loss: 4.6315 |  Task loss: 0.0966 | T: 235.45s
| It: 41001 | Rec. loss: 4.4336 |  Task loss: 0.0916 | T: 239.47s
| It: 42001 | Rec. loss: 4.2341 |  Task loss: 0.0916 | T: 243.93s
| It: 43001 | Rec. loss: 4.0440 |  Task loss: 0.0920 | T: 249.84s
| It: 44001 | Rec. loss: 3.8692 |  Task loss: 0.0875 | T: 244.92s
| It: 45001 | Rec. loss: 3.6513 |  Task loss: 0.0907 | T: 240.65s
| It: 46001 | Rec. loss: 3.4659 |  Task loss: 0.0909 | T: 243.97s
| It: 47001 | Rec. loss: 3.2777 |  Task loss: 0.0912 | T: 237.87s
| It: 48001 | Rec. loss: 3.0974 |  Task loss: 0.0888 | T: 241.59s
| It: 49001 | Rec. loss: 2.9200 |  Task loss: 0.0902 | T: 244.76s
| It: 50001 | Rec. loss: 2.7465 |  Task loss: 0.0888 | T: 249.74s
| It: 51001 | Rec. loss: 2.5718 |  Task loss: 0.0834 | T: 242.45s
| It: 52001 | Rec. loss: 2.4074 |  Task loss: 0.0831 | T: 241.77s
| It: 53001 | Rec. loss: 2.2421 |  Task loss: 0.0823 | T: 242.96s
| It: 54001 | Rec. loss: 2.1112 |  Task loss: 0.0822 | T: 249.14s
| It: 55001 | Rec. loss: 1.9837 |  Task loss: 0.0800 | T: 251.63s
| It: 56001 | Rec. loss: 1.8766 |  Task loss: 0.0803 | T: 239.59s
| It: 57001 | Rec. loss: 1.7701 |  Task loss: 0.0784 | T: 247.45s
| It: 58001 | Rec. loss: 1.6681 |  Task loss: 0.0798 | T: 248.16s
| It: 59001 | Rec. loss: 1.5898 |  Task loss: 0.0791 | T: 247.16s
| It: 60001 | Rec. loss: 1.5031 |  Task loss: 0.0778 | T: 244.04s
| It: 61001 | Rec. loss: 1.4324 |  Task loss: 0.0766 | T: 247.67s
| It: 62001 | Rec. loss: 1.3683 |  Task loss: 0.0756 | T: 238.83s
| It: 63001 | Rec. loss: 1.3009 |  Task loss: 0.0758 | T: 237.73s
| It: 64001 | Rec. loss: 1.2424 |  Task loss: 0.0743 | T: 241.53s
| It: 65001 | Rec. loss: 1.1852 |  Task loss: 0.0742 | T: 241.75s
| It: 66001 | Rec. loss: 1.1391 |  Task loss: 0.0730 | T: 237.02s
| It: 67001 | Rec. loss: 1.1018 |  Task loss: 0.0723 | T: 233.93s
| It: 68001 | Rec. loss: 1.0501 |  Task loss: 0.0713 | T: 236.41s
| It: 69001 | Rec. loss: 1.0139 |  Task loss: 0.0712 | T: 239.71s
| It: 70001 | Rec. loss: 0.9846 |  Task loss: 0.0710 | T: 243.74s
| It: 71001 | Rec. loss: 0.9513 |  Task loss: 0.0700 | T: 236.66s
| It: 72001 | Rec. loss: 0.9251 |  Task loss: 0.0698 | T: 240.85s
| It: 73001 | Rec. loss: 0.9028 |  Task loss: 0.0699 | T: 253.15s
| It: 74001 | Rec. loss: 0.8835 |  Task loss: 0.0697 | T: 236.16s
| It: 75001 | Rec. loss: 0.8674 |  Task loss: 0.0697 | T: 237.29s
| It: 76001 | Rec. loss: 0.8547 |  Task loss: 0.0695 | T: 243.25s
| It: 77001 | Rec. loss: 0.8445 |  Task loss: 0.0696 | T: 239.70s
| It: 78001 | Rec. loss: 0.8371 |  Task loss: 0.0696 | T: 235.92s
| It: 79001 | Rec. loss: 0.8331 |  Task loss: 0.0695 | T: 237.47s
| It: 80000 | Rec. loss: 0.8321 |  Task loss: 0.0695 | T: 241.31s
Optimal candidate solution with rec. loss 1926.2268 selected.
Reconstruction stats:
Starting evaluations for attack effectiveness report...
The size of tensor a (55) must match the size of tensor b (61) at non-singleton dimension 3
None
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_post1.png
========================================================================

Trial 2

Investigating use case small_batch_imagenet with server type honest_but_curious.
Seed: 8070
Model architecture vggface2 loaded with 27,910,327 parameters and 29,712 buffers.
Overall this is a data ratio of      23:1 for target shape [8, 3, 224, 224] given that num_queries=1.
User (of type UserSingleStep) with settings:
    Number of data points: 8

    Threat model:
    User provides labels: False
    User provides buffers: True
    User provides number of data points: True

    Data:
    Dataset: flickr_faces
    user: 0
    
        
Server (of type HonestServer) with settings:
    Threat model: Honest-but-curious
    Number of planned queries: 1
    Has external/public data: False

    Model:
        model specification: vggface2
        model state: default
        public buffers: False

    Secrets: {}
    
Attacker (of type OptimizationBasedAttacker) with settings:
    Hyperparameter Template: see-through-gradients

    Objective: Euclidean loss with scale=0.0001 and task reg=0.0
    Regularizers: Total Variation, scale=0.0001. p=1 q=1. 
                  Input L^p norm regularization, scale=1e-06, p=2
                  Deep Inversion Regularization (matching batch norms), scale=0.1, first-bn-mult=10
    Augmentations: 

    Optimization Setup:
        optimizer: adam
        signed: False
        step_size: 0.1
        boxed: True
        max_iterations: 80000
        step_size_decay: cosine-decay
        langevin_noise: 0.01
        warmup: 100
        grad_clip: None
        callback: 1000
        
Computing user update on user 0 in model mode: training.
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_pre2.png
Reconstructing user data...
Files already downloaded and verified
initial data len: 8
Recovered labels [121, 207, 265, 425, 466, 691, 789, 849] through strategy yin.
| It: 1 | Rec. loss: 33.1092 |  Task loss: 11.3968 | T: 0.24s
| It: 1001 | Rec. loss: 9.2899 |  Task loss: 0.1649 | T: 247.29s
| It: 2001 | Rec. loss: 8.8854 |  Task loss: 0.1616 | T: 248.48s
| It: 3001 | Rec. loss: 8.7827 |  Task loss: 0.1558 | T: 241.05s
| It: 4001 | Rec. loss: 8.6744 |  Task loss: 0.1656 | T: 240.95s
| It: 5001 | Rec. loss: 8.6490 |  Task loss: 0.1595 | T: 243.31s
| It: 6001 | Rec. loss: 8.5435 |  Task loss: 0.1570 | T: 241.60s
| It: 7001 | Rec. loss: 8.5489 |  Task loss: 0.1629 | T: 246.12s
| It: 8001 | Rec. loss: 8.4721 |  Task loss: 0.1487 | T: 239.66s
| It: 9001 | Rec. loss: 8.4425 |  Task loss: 0.1495 | T: 239.96s
| It: 10001 | Rec. loss: 8.3905 |  Task loss: 0.1501 | T: 241.55s
| It: 11001 | Rec. loss: 8.3189 |  Task loss: 0.1527 | T: 245.26s
| It: 12001 | Rec. loss: 8.2498 |  Task loss: 0.1503 | T: 240.28s
| It: 13001 | Rec. loss: 8.1946 |  Task loss: 0.1445 | T: 238.00s
| It: 14001 | Rec. loss: 8.1211 |  Task loss: 0.1431 | T: 234.32s
| It: 15001 | Rec. loss: 8.0867 |  Task loss: 0.1383 | T: 242.35s
| It: 16001 | Rec. loss: 7.9958 |  Task loss: 0.1352 | T: 237.92s
| It: 17001 | Rec. loss: 7.9305 |  Task loss: 0.1488 | T: 240.99s
| It: 18001 | Rec. loss: 7.8284 |  Task loss: 0.1428 | T: 244.64s
| It: 19001 | Rec. loss: 7.7450 |  Task loss: 0.1396 | T: 225.39s
| It: 20001 | Rec. loss: 7.6483 |  Task loss: 0.1443 | T: 243.75s
| It: 21001 | Rec. loss: 7.5810 |  Task loss: 0.1335 | T: 245.35s
| It: 22001 | Rec. loss: 7.4637 |  Task loss: 0.1428 | T: 241.40s
| It: 23001 | Rec. loss: 7.3608 |  Task loss: 0.1362 | T: 249.20s
| It: 24001 | Rec. loss: 7.2582 |  Task loss: 0.1426 | T: 248.23s
| It: 25001 | Rec. loss: 7.1315 |  Task loss: 0.1381 | T: 244.15s
| It: 26001 | Rec. loss: 6.9960 |  Task loss: 0.1278 | T: 241.92s
| It: 27001 | Rec. loss: 6.8761 |  Task loss: 0.1310 | T: 238.50s
| It: 28001 | Rec. loss: 6.7742 |  Task loss: 0.1354 | T: 236.04s
| It: 29001 | Rec. loss: 6.6533 |  Task loss: 0.1311 | T: 243.27s
| It: 30001 | Rec. loss: 6.4660 |  Task loss: 0.1315 | T: 242.48s
| It: 31001 | Rec. loss: 6.3479 |  Task loss: 0.1240 | T: 245.91s
| It: 32001 | Rec. loss: 6.1889 |  Task loss: 0.1305 | T: 237.70s
| It: 33001 | Rec. loss: 6.0791 |  Task loss: 0.1307 | T: 237.85s
| It: 34001 | Rec. loss: 5.8974 |  Task loss: 0.1252 | T: 239.94s
| It: 35001 | Rec. loss: 5.7172 |  Task loss: 0.1155 | T: 239.60s
| It: 36001 | Rec. loss: 5.5270 |  Task loss: 0.1141 | T: 244.12s
| It: 37001 | Rec. loss: 5.3628 |  Task loss: 0.1138 | T: 239.41s
| It: 38001 | Rec. loss: 5.1758 |  Task loss: 0.1125 | T: 243.32s
| It: 39001 | Rec. loss: 4.9990 |  Task loss: 0.1152 | T: 243.96s
| It: 40001 | Rec. loss: 4.7862 |  Task loss: 0.1141 | T: 244.71s
| It: 41001 | Rec. loss: 4.5979 |  Task loss: 0.1105 | T: 244.28s
| It: 42001 | Rec. loss: 4.4135 |  Task loss: 0.1041 | T: 260.66s
| It: 43001 | Rec. loss: 4.2228 |  Task loss: 0.1043 | T: 243.72s
| It: 44001 | Rec. loss: 4.0053 |  Task loss: 0.1024 | T: 240.50s
| It: 45001 | Rec. loss: 3.8196 |  Task loss: 0.1058 | T: 245.54s
| It: 46001 | Rec. loss: 3.6316 |  Task loss: 0.1058 | T: 235.47s
| It: 47001 | Rec. loss: 3.4388 |  Task loss: 0.1048 | T: 245.42s
| It: 48001 | Rec. loss: 3.2536 |  Task loss: 0.1013 | T: 240.24s
| It: 49001 | Rec. loss: 3.0798 |  Task loss: 0.1008 | T: 239.39s
| It: 50001 | Rec. loss: 2.9142 |  Task loss: 0.1004 | T: 243.23s
| It: 51001 | Rec. loss: 2.7789 |  Task loss: 0.0982 | T: 236.44s
| It: 52001 | Rec. loss: 2.6118 |  Task loss: 0.0946 | T: 237.38s
| It: 53001 | Rec. loss: 2.4752 |  Task loss: 0.0949 | T: 240.36s
| It: 54001 | Rec. loss: 2.3502 |  Task loss: 0.0938 | T: 243.68s
| It: 55001 | Rec. loss: 2.2267 |  Task loss: 0.0907 | T: 240.89s
| It: 56001 | Rec. loss: 2.1068 |  Task loss: 0.0937 | T: 243.37s
| It: 57001 | Rec. loss: 2.0006 |  Task loss: 0.0916 | T: 248.27s
| It: 58001 | Rec. loss: 1.9180 |  Task loss: 0.0900 | T: 245.29s
| It: 59001 | Rec. loss: 1.8085 |  Task loss: 0.0894 | T: 234.30s
| It: 60001 | Rec. loss: 1.7312 |  Task loss: 0.0869 | T: 241.33s
| It: 61001 | Rec. loss: 1.6799 |  Task loss: 0.0846 | T: 245.28s
| It: 62001 | Rec. loss: 1.5750 |  Task loss: 0.0846 | T: 233.69s
| It: 63001 | Rec. loss: 1.5138 |  Task loss: 0.0818 | T: 241.44s
| It: 64001 | Rec. loss: 1.4473 |  Task loss: 0.0827 | T: 239.81s
| It: 65001 | Rec. loss: 1.4050 |  Task loss: 0.0806 | T: 255.59s
| It: 66001 | Rec. loss: 1.3407 |  Task loss: 0.0810 | T: 242.30s
| It: 67001 | Rec. loss: 1.2988 |  Task loss: 0.0808 | T: 247.77s
| It: 68001 | Rec. loss: 1.2502 |  Task loss: 0.0797 | T: 248.51s
| It: 69001 | Rec. loss: 1.2131 |  Task loss: 0.0804 | T: 249.08s
| It: 70001 | Rec. loss: 1.1775 |  Task loss: 0.0796 | T: 243.99s
| It: 71001 | Rec. loss: 1.1450 |  Task loss: 0.0796 | T: 249.41s
| It: 72001 | Rec. loss: 1.1177 |  Task loss: 0.0795 | T: 248.38s
| It: 73001 | Rec. loss: 1.0943 |  Task loss: 0.0794 | T: 246.12s
| It: 74001 | Rec. loss: 1.0723 |  Task loss: 0.0795 | T: 249.79s
| It: 75001 | Rec. loss: 1.0542 |  Task loss: 0.0794 | T: 251.81s
| It: 76001 | Rec. loss: 1.0388 |  Task loss: 0.0797 | T: 244.14s
| It: 77001 | Rec. loss: 1.0265 |  Task loss: 0.0799 | T: 243.46s
| It: 78001 | Rec. loss: 1.0170 |  Task loss: 0.0800 | T: 245.09s
| It: 79001 | Rec. loss: 1.0116 |  Task loss: 0.0801 | T: 248.69s
| It: 80000 | Rec. loss: 1.0103 |  Task loss: 0.0801 | T: 241.46s
Optimal candidate solution with rec. loss 1919.1158 selected.
Reconstruction stats:
Starting evaluations for attack effectiveness report...
/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
The size of tensor a (55) must match the size of tensor b (61) at non-singleton dimension 3
None
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_post2.png
========================================================================

