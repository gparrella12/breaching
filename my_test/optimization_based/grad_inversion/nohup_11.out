Trial 0

Investigating use case small_batch_imagenet with server type honest_but_curious.
Seed: 74461
Model architecture vggface2 loaded with 27,910,327 parameters and 29,712 buffers.
Overall this is a data ratio of      23:1 for target shape [8, 3, 224, 224] given that num_queries=1.
User (of type UserSingleStep) with settings:
    Number of data points: 8

    Threat model:
    User provides labels: False
    User provides buffers: True
    User provides number of data points: True

    Data:
    Dataset: flickr_faces
    user: 0
    
        
Server (of type HonestServer) with settings:
    Threat model: Honest-but-curious
    Number of planned queries: 1
    Has external/public data: False

    Model:
        model specification: vggface2
        model state: default
        public buffers: False

    Secrets: {}
    
Attacker (of type OptimizationBasedAttacker) with settings:
    Hyperparameter Template: see-through-gradients

    Objective: Euclidean loss with scale=0.0001 and task reg=0.0
    Regularizers: Total Variation, scale=0.0001. p=1 q=1. 
                  Input L^p norm regularization, scale=1e-06, p=2
                  Deep Inversion Regularization (matching batch norms), scale=0.1, first-bn-mult=10
    Augmentations: 

    Optimization Setup:
        optimizer: adam
        signed: False
        step_size: 0.1
        boxed: True
        max_iterations: 160000
        step_size_decay: cosine-decay
        langevin_noise: 0.01
        warmup: 100
        grad_clip: None
        callback: 1000
        
Computing user update on user 0 in model mode: training.
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_pre0_1.png
Reconstructing user data...
Files already downloaded and verified
initial data len: 8
Recovered labels [36, 85, 389, 586, 744, 828, 895, 926] through strategy yin.
| It: 1 | Rec. loss: 32.6408 |  Task loss: 11.6023 | T: 1.54s
| It: 1001 | Rec. loss: 9.3905 |  Task loss: 0.3613 | T: 236.26s
| It: 2001 | Rec. loss: 9.1323 |  Task loss: 0.1921 | T: 226.11s
| It: 3001 | Rec. loss: 9.0353 |  Task loss: 0.1573 | T: 224.36s
| It: 4001 | Rec. loss: 8.8643 |  Task loss: 0.1604 | T: 223.67s
| It: 5001 | Rec. loss: 8.7885 |  Task loss: 0.1496 | T: 227.48s
| It: 6001 | Rec. loss: 8.8426 |  Task loss: 0.1548 | T: 228.67s
| It: 7001 | Rec. loss: 8.6954 |  Task loss: 0.1558 | T: 227.28s
| It: 8001 | Rec. loss: 8.6646 |  Task loss: 0.1538 | T: 230.17s
| It: 9001 | Rec. loss: 8.6260 |  Task loss: 0.1438 | T: 227.05s
| It: 10001 | Rec. loss: 8.7076 |  Task loss: 0.1533 | T: 236.95s
| It: 11001 | Rec. loss: 8.6359 |  Task loss: 0.1579 | T: 226.97s
| It: 12001 | Rec. loss: 8.5767 |  Task loss: 0.1469 | T: 231.69s
| It: 13001 | Rec. loss: 8.6281 |  Task loss: 0.1640 | T: 222.70s
| It: 14001 | Rec. loss: 8.5419 |  Task loss: 0.1564 | T: 222.64s
| It: 15001 | Rec. loss: 8.5620 |  Task loss: 0.1709 | T: 230.18s
| It: 16001 | Rec. loss: 8.5092 |  Task loss: 0.1612 | T: 229.76s
| It: 17001 | Rec. loss: 8.5001 |  Task loss: 0.1641 | T: 242.56s
| It: 18001 | Rec. loss: 8.4562 |  Task loss: 0.1569 | T: 229.91s
| It: 19001 | Rec. loss: 8.4684 |  Task loss: 0.1488 | T: 228.07s
| It: 20001 | Rec. loss: 8.4812 |  Task loss: 0.1656 | T: 228.68s
| It: 21001 | Rec. loss: 8.4068 |  Task loss: 0.1448 | T: 223.23s
| It: 22001 | Rec. loss: 8.3917 |  Task loss: 0.1531 | T: 226.54s
| It: 23001 | Rec. loss: 8.3658 |  Task loss: 0.1498 | T: 227.11s
| It: 24001 | Rec. loss: 8.3154 |  Task loss: 0.1483 | T: 224.65s
| It: 25001 | Rec. loss: 8.3265 |  Task loss: 0.1408 | T: 225.55s
| It: 26001 | Rec. loss: 8.2748 |  Task loss: 0.1495 | T: 227.11s
| It: 27001 | Rec. loss: 8.2178 |  Task loss: 0.1620 | T: 228.05s
| It: 28001 | Rec. loss: 8.1953 |  Task loss: 0.1437 | T: 228.61s
| It: 29001 | Rec. loss: 8.1679 |  Task loss: 0.1511 | T: 223.95s
| It: 30001 | Rec. loss: 8.1441 |  Task loss: 0.1490 | T: 233.29s
| It: 31001 | Rec. loss: 8.1273 |  Task loss: 0.1468 | T: 234.02s
| It: 32001 | Rec. loss: 8.0634 |  Task loss: 0.1465 | T: 227.01s
| It: 33001 | Rec. loss: 8.0023 |  Task loss: 0.1424 | T: 227.41s
| It: 34001 | Rec. loss: 7.9527 |  Task loss: 0.1522 | T: 226.50s
| It: 35001 | Rec. loss: 7.9364 |  Task loss: 0.1403 | T: 225.80s
| It: 36001 | Rec. loss: 7.9029 |  Task loss: 0.1469 | T: 234.56s
| It: 37001 | Rec. loss: 7.8772 |  Task loss: 0.1361 | T: 230.43s
| It: 38001 | Rec. loss: 7.8220 |  Task loss: 0.1335 | T: 231.30s
| It: 39001 | Rec. loss: 7.7659 |  Task loss: 0.1465 | T: 221.91s
| It: 40001 | Rec. loss: 7.7383 |  Task loss: 0.1460 | T: 233.35s
| It: 41001 | Rec. loss: 7.7220 |  Task loss: 0.1403 | T: 235.08s
| It: 42001 | Rec. loss: 7.6291 |  Task loss: 0.1390 | T: 229.59s
| It: 43001 | Rec. loss: 7.5807 |  Task loss: 0.1368 | T: 226.75s
| It: 44001 | Rec. loss: 7.5301 |  Task loss: 0.1403 | T: 225.93s
| It: 45001 | Rec. loss: 7.5152 |  Task loss: 0.1416 | T: 223.05s
| It: 46001 | Rec. loss: 7.4410 |  Task loss: 0.1396 | T: 225.96s
| It: 47001 | Rec. loss: 7.3500 |  Task loss: 0.1400 | T: 224.72s
| It: 48001 | Rec. loss: 7.3395 |  Task loss: 0.1428 | T: 223.67s
| It: 49001 | Rec. loss: 7.2414 |  Task loss: 0.1352 | T: 230.06s
| It: 50001 | Rec. loss: 7.2100 |  Task loss: 0.1326 | T: 228.81s
| It: 51001 | Rec. loss: 7.1439 |  Task loss: 0.1364 | T: 228.85s
| It: 52001 | Rec. loss: 7.0716 |  Task loss: 0.1337 | T: 228.96s
| It: 53001 | Rec. loss: 7.0438 |  Task loss: 0.1251 | T: 229.20s
| It: 54001 | Rec. loss: 6.9670 |  Task loss: 0.1318 | T: 227.85s
| It: 55001 | Rec. loss: 6.8986 |  Task loss: 0.1325 | T: 231.27s
| It: 56001 | Rec. loss: 6.8305 |  Task loss: 0.1358 | T: 222.46s
| It: 57001 | Rec. loss: 6.7622 |  Task loss: 0.1334 | T: 227.16s
| It: 58001 | Rec. loss: 6.7098 |  Task loss: 0.1311 | T: 230.28s
| It: 59001 | Rec. loss: 6.6368 |  Task loss: 0.1345 | T: 225.11s
| It: 60001 | Rec. loss: 6.5304 |  Task loss: 0.1315 | T: 227.72s
| It: 61001 | Rec. loss: 6.4663 |  Task loss: 0.1320 | T: 227.96s
| It: 62001 | Rec. loss: 6.3931 |  Task loss: 0.1265 | T: 224.95s
| It: 63001 | Rec. loss: 6.3002 |  Task loss: 0.1248 | T: 226.28s
| It: 64001 | Rec. loss: 6.2291 |  Task loss: 0.1240 | T: 230.87s
| It: 65001 | Rec. loss: 6.1443 |  Task loss: 0.1212 | T: 229.89s
| It: 66001 | Rec. loss: 6.0664 |  Task loss: 0.1192 | T: 244.05s
| It: 67001 | Rec. loss: 5.9869 |  Task loss: 0.1243 | T: 257.44s
| It: 68001 | Rec. loss: 5.8799 |  Task loss: 0.1235 | T: 255.35s
| It: 69001 | Rec. loss: 5.7986 |  Task loss: 0.1244 | T: 254.71s
| It: 70001 | Rec. loss: 5.7202 |  Task loss: 0.1197 | T: 250.53s
| It: 71001 | Rec. loss: 5.6454 |  Task loss: 0.1203 | T: 249.92s
| It: 72001 | Rec. loss: 5.5389 |  Task loss: 0.1168 | T: 238.65s
| It: 73001 | Rec. loss: 5.4667 |  Task loss: 0.1199 | T: 246.43s
| It: 74001 | Rec. loss: 5.3712 |  Task loss: 0.1103 | T: 228.49s
| It: 75001 | Rec. loss: 5.2699 |  Task loss: 0.1091 | T: 224.54s
| It: 76001 | Rec. loss: 5.1561 |  Task loss: 0.1108 | T: 230.29s
| It: 77001 | Rec. loss: 5.0780 |  Task loss: 0.1060 | T: 226.78s
| It: 78001 | Rec. loss: 4.9609 |  Task loss: 0.1094 | T: 226.50s
| It: 79001 | Rec. loss: 4.8646 |  Task loss: 0.1091 | T: 227.43s
| It: 80001 | Rec. loss: 4.7533 |  Task loss: 0.1015 | T: 225.56s
| It: 81001 | Rec. loss: 4.6423 |  Task loss: 0.1009 | T: 226.35s
| It: 82001 | Rec. loss: 4.5525 |  Task loss: 0.1023 | T: 226.59s
| It: 83001 | Rec. loss: 4.4490 |  Task loss: 0.1004 | T: 229.36s
| It: 84001 | Rec. loss: 4.3457 |  Task loss: 0.1008 | T: 224.80s
| It: 85001 | Rec. loss: 4.2376 |  Task loss: 0.1016 | T: 224.89s
| It: 86001 | Rec. loss: 4.1419 |  Task loss: 0.1028 | T: 226.06s
| It: 87001 | Rec. loss: 4.0382 |  Task loss: 0.0984 | T: 231.84s
| It: 88001 | Rec. loss: 3.9348 |  Task loss: 0.0975 | T: 229.05s
| It: 89001 | Rec. loss: 3.8377 |  Task loss: 0.0977 | T: 226.82s
| It: 90001 | Rec. loss: 3.7346 |  Task loss: 0.0977 | T: 227.13s
| It: 91001 | Rec. loss: 3.6361 |  Task loss: 0.0964 | T: 227.58s
| It: 92001 | Rec. loss: 3.5351 |  Task loss: 0.0960 | T: 228.78s
| It: 93001 | Rec. loss: 3.4531 |  Task loss: 0.0946 | T: 230.19s
| It: 94001 | Rec. loss: 3.3601 |  Task loss: 0.0918 | T: 229.93s
| It: 95001 | Rec. loss: 3.2822 |  Task loss: 0.0961 | T: 230.19s
| It: 96001 | Rec. loss: 3.1833 |  Task loss: 0.0965 | T: 226.03s
| It: 97001 | Rec. loss: 3.0981 |  Task loss: 0.0956 | T: 223.37s
| It: 98001 | Rec. loss: 3.0232 |  Task loss: 0.0938 | T: 223.66s
| It: 99001 | Rec. loss: 2.9455 |  Task loss: 0.0944 | T: 226.88s
| It: 100001 | Rec. loss: 2.8694 |  Task loss: 0.0935 | T: 226.18s
| It: 101001 | Rec. loss: 2.7958 |  Task loss: 0.0915 | T: 230.59s
| It: 102001 | Rec. loss: 2.7200 |  Task loss: 0.0901 | T: 226.86s
| It: 103001 | Rec. loss: 2.6493 |  Task loss: 0.0892 | T: 225.54s
| It: 104001 | Rec. loss: 2.5812 |  Task loss: 0.0895 | T: 230.27s
| It: 105001 | Rec. loss: 2.5082 |  Task loss: 0.0871 | T: 225.90s
| It: 106001 | Rec. loss: 2.4484 |  Task loss: 0.0859 | T: 227.72s
| It: 107001 | Rec. loss: 2.3764 |  Task loss: 0.0874 | T: 230.97s
| It: 108001 | Rec. loss: 2.3066 |  Task loss: 0.0845 | T: 230.47s
| It: 109001 | Rec. loss: 2.2433 |  Task loss: 0.0830 | T: 221.17s
| It: 110001 | Rec. loss: 2.1852 |  Task loss: 0.0831 | T: 232.28s
| It: 111001 | Rec. loss: 2.1294 |  Task loss: 0.0821 | T: 231.07s
| It: 112001 | Rec. loss: 2.0673 |  Task loss: 0.0816 | T: 230.79s
| It: 113001 | Rec. loss: 2.0113 |  Task loss: 0.0828 | T: 224.89s
| It: 114001 | Rec. loss: 1.9594 |  Task loss: 0.0805 | T: 223.95s
| It: 115001 | Rec. loss: 1.9036 |  Task loss: 0.0820 | T: 222.65s
| It: 116001 | Rec. loss: 1.8549 |  Task loss: 0.0803 | T: 232.55s
| It: 117001 | Rec. loss: 1.7973 |  Task loss: 0.0812 | T: 231.82s
| It: 118001 | Rec. loss: 1.7510 |  Task loss: 0.0799 | T: 225.49s
| It: 119001 | Rec. loss: 1.6992 |  Task loss: 0.0795 | T: 224.32s
| It: 120001 | Rec. loss: 1.6542 |  Task loss: 0.0798 | T: 226.31s
| It: 121001 | Rec. loss: 1.6047 |  Task loss: 0.0778 | T: 227.33s
| It: 122001 | Rec. loss: 1.5669 |  Task loss: 0.0805 | T: 228.40s
| It: 123001 | Rec. loss: 1.5231 |  Task loss: 0.0790 | T: 227.31s
| It: 124001 | Rec. loss: 1.4761 |  Task loss: 0.0790 | T: 227.22s
| It: 125001 | Rec. loss: 1.4327 |  Task loss: 0.0781 | T: 227.94s
| It: 126001 | Rec. loss: 1.3918 |  Task loss: 0.0790 | T: 233.37s
| It: 127001 | Rec. loss: 1.3542 |  Task loss: 0.0771 | T: 221.52s
| It: 128001 | Rec. loss: 1.3121 |  Task loss: 0.0761 | T: 221.32s
| It: 129001 | Rec. loss: 1.2702 |  Task loss: 0.0772 | T: 227.59s
| It: 130001 | Rec. loss: 1.2381 |  Task loss: 0.0764 | T: 231.57s
| It: 131001 | Rec. loss: 1.2010 |  Task loss: 0.0761 | T: 244.05s
| It: 132001 | Rec. loss: 1.1674 |  Task loss: 0.0765 | T: 230.92s
| It: 133001 | Rec. loss: 1.1278 |  Task loss: 0.0751 | T: 226.30s
| It: 134001 | Rec. loss: 1.0988 |  Task loss: 0.0754 | T: 232.03s
| It: 135001 | Rec. loss: 1.0606 |  Task loss: 0.0748 | T: 229.51s
| It: 136001 | Rec. loss: 1.0292 |  Task loss: 0.0748 | T: 238.82s
| It: 137001 | Rec. loss: 1.0007 |  Task loss: 0.0745 | T: 229.99s
| It: 138001 | Rec. loss: 0.9753 |  Task loss: 0.0742 | T: 227.73s
| It: 139001 | Rec. loss: 0.9492 |  Task loss: 0.0744 | T: 230.15s
| It: 140001 | Rec. loss: 0.9173 |  Task loss: 0.0735 | T: 225.22s
| It: 141001 | Rec. loss: 0.8931 |  Task loss: 0.0738 | T: 228.39s
| It: 142001 | Rec. loss: 0.8657 |  Task loss: 0.0737 | T: 249.04s
| It: 143001 | Rec. loss: 0.8435 |  Task loss: 0.0737 | T: 245.88s
| It: 144001 | Rec. loss: 0.8248 |  Task loss: 0.0740 | T: 232.33s
| It: 145001 | Rec. loss: 0.8040 |  Task loss: 0.0740 | T: 225.42s
| It: 146001 | Rec. loss: 0.7875 |  Task loss: 0.0738 | T: 223.51s
| It: 147001 | Rec. loss: 0.7732 |  Task loss: 0.0738 | T: 227.61s
| It: 148001 | Rec. loss: 0.7567 |  Task loss: 0.0737 | T: 227.41s
| It: 149001 | Rec. loss: 0.7424 |  Task loss: 0.0736 | T: 229.46s
| It: 150001 | Rec. loss: 0.7281 |  Task loss: 0.0736 | T: 237.48s
| It: 151001 | Rec. loss: 0.7139 |  Task loss: 0.0737 | T: 229.86s
| It: 152001 | Rec. loss: 0.7032 |  Task loss: 0.0737 | T: 229.56s
| It: 153001 | Rec. loss: 0.6936 |  Task loss: 0.0736 | T: 231.24s
| It: 154001 | Rec. loss: 0.6857 |  Task loss: 0.0737 | T: 232.71s
| It: 155001 | Rec. loss: 0.6791 |  Task loss: 0.0737 | T: 227.22s
| It: 156001 | Rec. loss: 0.6736 |  Task loss: 0.0737 | T: 222.85s
| It: 157001 | Rec. loss: 0.6694 |  Task loss: 0.0737 | T: 223.90s
| It: 158001 | Rec. loss: 0.6666 |  Task loss: 0.0738 | T: 229.02s
| It: 159001 | Rec. loss: 0.6652 |  Task loss: 0.0738 | T: 227.46s
| It: 160000 | Rec. loss: 0.6649 |  Task loss: 0.0738 | T: 226.34s
Optimal candidate solution with rec. loss 2233.3333 selected.
Reconstruction stats:
Starting evaluations for attack effectiveness report...
The size of tensor a (55) must match the size of tensor b (61) at non-singleton dimension 3
None
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_post0_1.png
========================================================================

Trial 1

Investigating use case small_batch_imagenet with server type honest_but_curious.
Seed: 5883
Model architecture vggface2 loaded with 27,910,327 parameters and 29,712 buffers.
Overall this is a data ratio of      23:1 for target shape [8, 3, 224, 224] given that num_queries=1.
User (of type UserSingleStep) with settings:
    Number of data points: 8

    Threat model:
    User provides labels: False
    User provides buffers: True
    User provides number of data points: True

    Data:
    Dataset: flickr_faces
    user: 0
    
        
Server (of type HonestServer) with settings:
    Threat model: Honest-but-curious
    Number of planned queries: 1
    Has external/public data: False

    Model:
        model specification: vggface2
        model state: default
        public buffers: False

    Secrets: {}
    
Attacker (of type OptimizationBasedAttacker) with settings:
    Hyperparameter Template: see-through-gradients

    Objective: Euclidean loss with scale=0.0001 and task reg=0.0
    Regularizers: Total Variation, scale=0.0001. p=1 q=1. 
                  Input L^p norm regularization, scale=1e-06, p=2
                  Deep Inversion Regularization (matching batch norms), scale=0.1, first-bn-mult=10
    Augmentations: 

    Optimization Setup:
        optimizer: adam
        signed: False
        step_size: 0.1
        boxed: True
        max_iterations: 160000
        step_size_decay: cosine-decay
        langevin_noise: 0.01
        warmup: 100
        grad_clip: None
        callback: 1000
        
Computing user update on user 0 in model mode: training.
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_pre1_1.png
Reconstructing user data...
Files already downloaded and verified
initial data len: 8
Recovered labels [258, 359, 383, 462, 486, 543, 650, 908] through strategy yin.
| It: 1 | Rec. loss: 29.5717 |  Task loss: 10.7574 | T: 0.26s
| It: 1001 | Rec. loss: 8.5792 |  Task loss: 0.1496 | T: 228.77s
| It: 2001 | Rec. loss: 8.3657 |  Task loss: 0.1399 | T: 228.32s
| It: 3001 | Rec. loss: 8.2587 |  Task loss: 0.1323 | T: 224.03s
| It: 4001 | Rec. loss: 8.2864 |  Task loss: 0.1275 | T: 228.46s
| It: 5001 | Rec. loss: 8.2049 |  Task loss: 0.1274 | T: 243.47s
| It: 6001 | Rec. loss: 8.1581 |  Task loss: 0.1247 | T: 241.24s
| It: 7001 | Rec. loss: 8.1968 |  Task loss: 0.1322 | T: 245.42s
| It: 8001 | Rec. loss: 8.1641 |  Task loss: 0.1293 | T: 266.56s
| It: 9001 | Rec. loss: 8.1264 |  Task loss: 0.1402 | T: 246.03s
| It: 10001 | Rec. loss: 8.1129 |  Task loss: 0.1385 | T: 239.21s
| It: 11001 | Rec. loss: 8.0979 |  Task loss: 0.1275 | T: 242.04s
| It: 12001 | Rec. loss: 8.0573 |  Task loss: 0.1282 | T: 242.85s
| It: 13001 | Rec. loss: 8.0987 |  Task loss: 0.1206 | T: 249.70s
| It: 14001 | Rec. loss: 8.0383 |  Task loss: 0.1264 | T: 244.21s
| It: 15001 | Rec. loss: 8.0029 |  Task loss: 0.1248 | T: 243.54s
| It: 16001 | Rec. loss: 7.9930 |  Task loss: 0.1300 | T: 244.29s
| It: 17001 | Rec. loss: 8.0148 |  Task loss: 0.1232 | T: 250.18s
| It: 18001 | Rec. loss: 7.9576 |  Task loss: 0.1187 | T: 243.36s
| It: 19001 | Rec. loss: 7.8951 |  Task loss: 0.1262 | T: 248.73s
| It: 20001 | Rec. loss: 7.8958 |  Task loss: 0.1216 | T: 231.66s
| It: 21001 | Rec. loss: 7.8732 |  Task loss: 0.1176 | T: 227.51s
| It: 22001 | Rec. loss: 7.8636 |  Task loss: 0.1175 | T: 229.98s
| It: 23001 | Rec. loss: 7.8686 |  Task loss: 0.1222 | T: 235.69s
| It: 24001 | Rec. loss: 7.7985 |  Task loss: 0.1122 | T: 232.30s
| It: 25001 | Rec. loss: 7.7401 |  Task loss: 0.1176 | T: 230.03s
| It: 26001 | Rec. loss: 7.7116 |  Task loss: 0.1229 | T: 238.89s
| It: 27001 | Rec. loss: 7.7356 |  Task loss: 0.1147 | T: 227.84s
| It: 28001 | Rec. loss: 7.6701 |  Task loss: 0.1181 | T: 219.85s
| It: 29001 | Rec. loss: 7.6447 |  Task loss: 0.1227 | T: 228.85s
| It: 30001 | Rec. loss: 7.6283 |  Task loss: 0.1213 | T: 229.28s
| It: 31001 | Rec. loss: 7.5875 |  Task loss: 0.1237 | T: 229.57s
| It: 32001 | Rec. loss: 7.5437 |  Task loss: 0.1164 | T: 234.80s
| It: 33001 | Rec. loss: 7.5013 |  Task loss: 0.1183 | T: 236.00s
| It: 34001 | Rec. loss: 7.4854 |  Task loss: 0.1109 | T: 227.76s
| It: 35001 | Rec. loss: 7.4634 |  Task loss: 0.1191 | T: 231.71s
| It: 36001 | Rec. loss: 7.4087 |  Task loss: 0.1105 | T: 228.61s
| It: 37001 | Rec. loss: 7.3513 |  Task loss: 0.1153 | T: 225.59s
| It: 38001 | Rec. loss: 7.2986 |  Task loss: 0.1166 | T: 222.10s
| It: 39001 | Rec. loss: 7.2934 |  Task loss: 0.1154 | T: 216.54s
| It: 40001 | Rec. loss: 7.2317 |  Task loss: 0.1138 | T: 249.37s
| It: 41001 | Rec. loss: 7.1704 |  Task loss: 0.1144 | T: 230.09s
| It: 42001 | Rec. loss: 7.0742 |  Task loss: 0.1048 | T: 244.10s
| It: 43001 | Rec. loss: 7.0631 |  Task loss: 0.1065 | T: 242.54s
| It: 44001 | Rec. loss: 7.0326 |  Task loss: 0.1080 | T: 240.90s
| It: 45001 | Rec. loss: 6.9539 |  Task loss: 0.1121 | T: 248.23s
| It: 46001 | Rec. loss: 6.9092 |  Task loss: 0.1112 | T: 252.64s
| It: 47001 | Rec. loss: 6.8424 |  Task loss: 0.1101 | T: 259.05s
| It: 48001 | Rec. loss: 6.8194 |  Task loss: 0.1120 | T: 257.91s
| It: 49001 | Rec. loss: 6.7584 |  Task loss: 0.1128 | T: 253.06s
| It: 50001 | Rec. loss: 6.7164 |  Task loss: 0.1060 | T: 249.57s
| It: 51001 | Rec. loss: 6.6359 |  Task loss: 0.1030 | T: 251.55s
| It: 52001 | Rec. loss: 6.5640 |  Task loss: 0.1011 | T: 248.84s
| It: 53001 | Rec. loss: 6.5272 |  Task loss: 0.1021 | T: 255.24s
| It: 54001 | Rec. loss: 6.4555 |  Task loss: 0.0983 | T: 255.68s
| It: 55001 | Rec. loss: 6.4198 |  Task loss: 0.0975 | T: 250.65s
| It: 56001 | Rec. loss: 6.3213 |  Task loss: 0.1019 | T: 249.57s
| It: 57001 | Rec. loss: 6.2524 |  Task loss: 0.1019 | T: 255.45s
| It: 58001 | Rec. loss: 6.1877 |  Task loss: 0.1024 | T: 248.24s
| It: 59001 | Rec. loss: 6.0950 |  Task loss: 0.1051 | T: 251.15s
| It: 60001 | Rec. loss: 6.0342 |  Task loss: 0.1000 | T: 254.95s
| It: 61001 | Rec. loss: 5.9681 |  Task loss: 0.0988 | T: 255.78s
| It: 62001 | Rec. loss: 5.9012 |  Task loss: 0.0999 | T: 253.44s
| It: 63001 | Rec. loss: 5.8544 |  Task loss: 0.1005 | T: 248.68s
| It: 64001 | Rec. loss: 5.7406 |  Task loss: 0.0941 | T: 253.70s
| It: 65001 | Rec. loss: 5.6690 |  Task loss: 0.0948 | T: 249.23s
| It: 66001 | Rec. loss: 5.5995 |  Task loss: 0.0988 | T: 247.62s
| It: 67001 | Rec. loss: 5.5104 |  Task loss: 0.0965 | T: 236.07s
| It: 68001 | Rec. loss: 5.4347 |  Task loss: 0.1001 | T: 223.82s
| It: 69001 | Rec. loss: 5.3359 |  Task loss: 0.0999 | T: 232.57s
| It: 70001 | Rec. loss: 5.2725 |  Task loss: 0.0954 | T: 227.95s
| It: 71001 | Rec. loss: 5.1795 |  Task loss: 0.0900 | T: 232.67s
| It: 72001 | Rec. loss: 5.1069 |  Task loss: 0.0951 | T: 224.86s
| It: 73001 | Rec. loss: 4.9936 |  Task loss: 0.0969 | T: 224.68s
| It: 74001 | Rec. loss: 4.9033 |  Task loss: 0.0876 | T: 225.64s
| It: 75001 | Rec. loss: 4.8179 |  Task loss: 0.0888 | T: 220.21s
| It: 76001 | Rec. loss: 4.7336 |  Task loss: 0.0907 | T: 222.39s
| It: 77001 | Rec. loss: 4.6321 |  Task loss: 0.0882 | T: 227.75s
| It: 78001 | Rec. loss: 4.5387 |  Task loss: 0.0862 | T: 231.86s
| It: 79001 | Rec. loss: 4.4576 |  Task loss: 0.0824 | T: 234.01s
| It: 80001 | Rec. loss: 4.3593 |  Task loss: 0.0898 | T: 220.72s
| It: 81001 | Rec. loss: 4.2556 |  Task loss: 0.0918 | T: 229.31s
| It: 82001 | Rec. loss: 4.2022 |  Task loss: 0.0884 | T: 226.38s
| It: 83001 | Rec. loss: 4.0829 |  Task loss: 0.0882 | T: 220.93s
| It: 84001 | Rec. loss: 3.9712 |  Task loss: 0.0854 | T: 225.54s
| It: 85001 | Rec. loss: 3.8678 |  Task loss: 0.0864 | T: 227.82s
| It: 86001 | Rec. loss: 3.8048 |  Task loss: 0.0837 | T: 227.11s
| It: 87001 | Rec. loss: 3.7210 |  Task loss: 0.0827 | T: 231.94s
| It: 88001 | Rec. loss: 3.6173 |  Task loss: 0.0788 | T: 224.22s
| It: 89001 | Rec. loss: 3.5269 |  Task loss: 0.0795 | T: 226.99s
| It: 90001 | Rec. loss: 3.4528 |  Task loss: 0.0822 | T: 221.15s
| It: 91001 | Rec. loss: 3.3858 |  Task loss: 0.0778 | T: 221.47s
| It: 92001 | Rec. loss: 3.2670 |  Task loss: 0.0774 | T: 227.59s
| It: 93001 | Rec. loss: 3.1947 |  Task loss: 0.0769 | T: 222.81s
| It: 94001 | Rec. loss: 3.0992 |  Task loss: 0.0805 | T: 222.02s
| It: 95001 | Rec. loss: 3.0204 |  Task loss: 0.0741 | T: 223.13s
| It: 96001 | Rec. loss: 2.9514 |  Task loss: 0.0744 | T: 226.27s
| It: 97001 | Rec. loss: 2.8591 |  Task loss: 0.0755 | T: 232.21s
| It: 98001 | Rec. loss: 2.7884 |  Task loss: 0.0750 | T: 222.26s
| It: 99001 | Rec. loss: 2.7255 |  Task loss: 0.0717 | T: 222.47s
| It: 100001 | Rec. loss: 2.6333 |  Task loss: 0.0698 | T: 231.95s
| It: 101001 | Rec. loss: 2.5599 |  Task loss: 0.0692 | T: 227.09s
| It: 102001 | Rec. loss: 2.4828 |  Task loss: 0.0708 | T: 226.11s
| It: 103001 | Rec. loss: 2.4048 |  Task loss: 0.0717 | T: 225.67s
| It: 104001 | Rec. loss: 2.3439 |  Task loss: 0.0707 | T: 226.82s
| It: 105001 | Rec. loss: 2.2726 |  Task loss: 0.0694 | T: 225.87s
| It: 106001 | Rec. loss: 2.2121 |  Task loss: 0.0680 | T: 234.14s
| It: 107001 | Rec. loss: 2.1407 |  Task loss: 0.0689 | T: 234.08s
| It: 108001 | Rec. loss: 2.0770 |  Task loss: 0.0697 | T: 225.90s
| It: 109001 | Rec. loss: 2.0272 |  Task loss: 0.0694 | T: 228.57s
| It: 110001 | Rec. loss: 1.9550 |  Task loss: 0.0690 | T: 230.54s
| It: 111001 | Rec. loss: 1.8869 |  Task loss: 0.0699 | T: 223.47s
| It: 112001 | Rec. loss: 1.8355 |  Task loss: 0.0687 | T: 228.80s
| It: 113001 | Rec. loss: 1.7776 |  Task loss: 0.0675 | T: 222.66s
| It: 114001 | Rec. loss: 1.7219 |  Task loss: 0.0676 | T: 226.88s
| It: 115001 | Rec. loss: 1.6697 |  Task loss: 0.0677 | T: 228.58s
| It: 116001 | Rec. loss: 1.6160 |  Task loss: 0.0672 | T: 230.46s
| It: 117001 | Rec. loss: 1.5704 |  Task loss: 0.0679 | T: 237.38s
| It: 118001 | Rec. loss: 1.5196 |  Task loss: 0.0687 | T: 222.93s
| It: 119001 | Rec. loss: 1.4723 |  Task loss: 0.0663 | T: 227.28s
| It: 120001 | Rec. loss: 1.4271 |  Task loss: 0.0668 | T: 229.26s
| It: 121001 | Rec. loss: 1.3803 |  Task loss: 0.0665 | T: 228.95s
| It: 122001 | Rec. loss: 1.3422 |  Task loss: 0.0653 | T: 225.04s
| It: 123001 | Rec. loss: 1.2977 |  Task loss: 0.0642 | T: 235.31s
| It: 124001 | Rec. loss: 1.2550 |  Task loss: 0.0646 | T: 245.87s
| It: 125001 | Rec. loss: 1.2162 |  Task loss: 0.0638 | T: 240.74s
| It: 126001 | Rec. loss: 1.1763 |  Task loss: 0.0630 | T: 235.97s
| It: 127001 | Rec. loss: 1.1374 |  Task loss: 0.0627 | T: 247.32s
| It: 128001 | Rec. loss: 1.1063 |  Task loss: 0.0630 | T: 244.90s
| It: 129001 | Rec. loss: 1.0685 |  Task loss: 0.0623 | T: 239.16s
| It: 130001 | Rec. loss: 1.0347 |  Task loss: 0.0621 | T: 245.38s
| It: 131001 | Rec. loss: 1.0040 |  Task loss: 0.0624 | T: 253.22s
| It: 132001 | Rec. loss: 0.9727 |  Task loss: 0.0620 | T: 248.33s
| It: 133001 | Rec. loss: 0.9375 |  Task loss: 0.0616 | T: 251.57s
| It: 134001 | Rec. loss: 0.9121 |  Task loss: 0.0617 | T: 243.36s
| It: 135001 | Rec. loss: 0.8823 |  Task loss: 0.0617 | T: 253.23s
| It: 136001 | Rec. loss: 0.8547 |  Task loss: 0.0613 | T: 248.72s
| It: 137001 | Rec. loss: 0.8339 |  Task loss: 0.0613 | T: 246.53s
| It: 138001 | Rec. loss: 0.8074 |  Task loss: 0.0611 | T: 246.56s
| It: 139001 | Rec. loss: 0.7833 |  Task loss: 0.0608 | T: 253.54s
| It: 140001 | Rec. loss: 0.7664 |  Task loss: 0.0606 | T: 231.59s
| It: 141001 | Rec. loss: 0.7466 |  Task loss: 0.0599 | T: 222.89s
| It: 142001 | Rec. loss: 0.7270 |  Task loss: 0.0591 | T: 231.93s
| It: 143001 | Rec. loss: 0.7130 |  Task loss: 0.0595 | T: 231.95s
| It: 144001 | Rec. loss: 0.6919 |  Task loss: 0.0588 | T: 226.65s
| It: 145001 | Rec. loss: 0.6766 |  Task loss: 0.0587 | T: 222.13s
| It: 146001 | Rec. loss: 0.6672 |  Task loss: 0.0583 | T: 225.10s
| It: 147001 | Rec. loss: 0.6520 |  Task loss: 0.0579 | T: 229.35s
| It: 148001 | Rec. loss: 0.6434 |  Task loss: 0.0581 | T: 227.56s
| It: 149001 | Rec. loss: 0.6323 |  Task loss: 0.0580 | T: 228.35s
| It: 150001 | Rec. loss: 0.6238 |  Task loss: 0.0578 | T: 233.62s
| It: 151001 | Rec. loss: 0.6172 |  Task loss: 0.0578 | T: 227.42s
| It: 152001 | Rec. loss: 0.6103 |  Task loss: 0.0577 | T: 222.01s
| It: 153001 | Rec. loss: 0.6047 |  Task loss: 0.0578 | T: 235.10s
| It: 154001 | Rec. loss: 0.5995 |  Task loss: 0.0577 | T: 223.24s
| It: 155001 | Rec. loss: 0.5953 |  Task loss: 0.0577 | T: 231.24s
| It: 156001 | Rec. loss: 0.5919 |  Task loss: 0.0578 | T: 227.45s
| It: 157001 | Rec. loss: 0.5891 |  Task loss: 0.0577 | T: 231.28s
| It: 158001 | Rec. loss: 0.5872 |  Task loss: 0.0577 | T: 230.10s
| It: 159001 | Rec. loss: 0.5862 |  Task loss: 0.0577 | T: 222.13s
| It: 160000 | Rec. loss: 0.5860 |  Task loss: 0.0577 | T: 222.11s
Optimal candidate solution with rec. loss 2166.9309 selected.
Reconstruction stats:
Starting evaluations for attack effectiveness report...
The size of tensor a (55) must match the size of tensor b (61) at non-singleton dimension 3
None
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_post1_1.png
========================================================================

Trial 2

Investigating use case small_batch_imagenet with server type honest_but_curious.
Seed: 42937
Model architecture vggface2 loaded with 27,910,327 parameters and 29,712 buffers.
Overall this is a data ratio of      23:1 for target shape [8, 3, 224, 224] given that num_queries=1.
User (of type UserSingleStep) with settings:
    Number of data points: 8

    Threat model:
    User provides labels: False
    User provides buffers: True
    User provides number of data points: True

    Data:
    Dataset: flickr_faces
    user: 0
    
        
Server (of type HonestServer) with settings:
    Threat model: Honest-but-curious
    Number of planned queries: 1
    Has external/public data: False

    Model:
        model specification: vggface2
        model state: default
        public buffers: False

    Secrets: {}
    
Attacker (of type OptimizationBasedAttacker) with settings:
    Hyperparameter Template: see-through-gradients

    Objective: Euclidean loss with scale=0.0001 and task reg=0.0
    Regularizers: Total Variation, scale=0.0001. p=1 q=1. 
                  Input L^p norm regularization, scale=1e-06, p=2
                  Deep Inversion Regularization (matching batch norms), scale=0.1, first-bn-mult=10
    Augmentations: 

    Optimization Setup:
        optimizer: adam
        signed: False
        step_size: 0.1
        boxed: True
        max_iterations: 160000
        step_size_decay: cosine-decay
        langevin_noise: 0.01
        warmup: 100
        grad_clip: None
        callback: 1000
        
Computing user update on user 0 in model mode: training.
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_pre2_1.png
Reconstructing user data...
Files already downloaded and verified
initial data len: 8
Recovered labels [39, 91, 121, 195, 386, 525, 605, 891] through strategy yin.
| It: 1 | Rec. loss: 31.7044 |  Task loss: 11.0381 | T: 0.23s
| It: 1001 | Rec. loss: 9.3266 |  Task loss: 0.7731 | T: 222.30s
| It: 2001 | Rec. loss: 9.0329 |  Task loss: 0.7719 | T: 226.63s
| It: 3001 | Rec. loss: 8.8422 |  Task loss: 0.7494 | T: 238.38s
| It: 4001 | Rec. loss: 8.7431 |  Task loss: 0.7147 | T: 230.26s
| It: 5001 | Rec. loss: 8.7130 |  Task loss: 0.7557 | T: 226.85s
| It: 6001 | Rec. loss: 8.6882 |  Task loss: 0.8291 | T: 230.21s
| It: 7001 | Rec. loss: 8.6518 |  Task loss: 0.8226 | T: 225.62s
| It: 8001 | Rec. loss: 8.6012 |  Task loss: 0.8547 | T: 229.72s
| It: 9001 | Rec. loss: 8.6253 |  Task loss: 0.7747 | T: 231.59s
| It: 10001 | Rec. loss: 8.5809 |  Task loss: 0.8161 | T: 228.96s
| It: 11001 | Rec. loss: 8.5794 |  Task loss: 0.7840 | T: 221.62s
| It: 12001 | Rec. loss: 8.5323 |  Task loss: 0.8191 | T: 239.66s
| It: 13001 | Rec. loss: 8.5437 |  Task loss: 0.8258 | T: 230.78s
| It: 14001 | Rec. loss: 8.4768 |  Task loss: 0.7251 | T: 227.46s
| It: 15001 | Rec. loss: 8.4559 |  Task loss: 0.7704 | T: 232.92s
| It: 16001 | Rec. loss: 8.4122 |  Task loss: 0.7578 | T: 228.33s
| It: 17001 | Rec. loss: 8.4414 |  Task loss: 0.7412 | T: 224.10s
| It: 18001 | Rec. loss: 8.4174 |  Task loss: 0.8268 | T: 229.54s
| It: 19001 | Rec. loss: 8.3705 |  Task loss: 0.7454 | T: 226.37s
| It: 20001 | Rec. loss: 8.3669 |  Task loss: 0.6541 | T: 231.47s
| It: 21001 | Rec. loss: 8.3336 |  Task loss: 0.6141 | T: 230.38s
| It: 22001 | Rec. loss: 8.3112 |  Task loss: 0.5127 | T: 226.95s
| It: 23001 | Rec. loss: 8.2954 |  Task loss: 0.1999 | T: 223.56s
| It: 24001 | Rec. loss: 8.2305 |  Task loss: 0.1816 | T: 223.18s
| It: 25001 | Rec. loss: 8.2480 |  Task loss: 0.1790 | T: 224.47s
| It: 26001 | Rec. loss: 8.2423 |  Task loss: 0.1688 | T: 223.77s
| It: 27001 | Rec. loss: 8.1833 |  Task loss: 0.1745 | T: 226.54s
| It: 28001 | Rec. loss: 8.1089 |  Task loss: 0.1695 | T: 223.96s
| It: 29001 | Rec. loss: 8.0833 |  Task loss: 0.1786 | T: 226.28s
| It: 30001 | Rec. loss: 8.0253 |  Task loss: 0.1710 | T: 225.56s
| It: 31001 | Rec. loss: 8.0094 |  Task loss: 0.1794 | T: 238.84s
| It: 32001 | Rec. loss: 8.0323 |  Task loss: 0.1854 | T: 231.38s
| It: 33001 | Rec. loss: 7.9221 |  Task loss: 0.1787 | T: 223.05s
| It: 34001 | Rec. loss: 7.9663 |  Task loss: 0.1673 | T: 225.39s
| It: 35001 | Rec. loss: 7.8391 |  Task loss: 0.1785 | T: 238.93s
| It: 36001 | Rec. loss: 7.8201 |  Task loss: 0.1603 | T: 228.89s
| It: 37001 | Rec. loss: 7.7883 |  Task loss: 0.1620 | T: 233.58s
| It: 38001 | Rec. loss: 7.7586 |  Task loss: 0.1520 | T: 229.41s
| It: 39001 | Rec. loss: 7.6802 |  Task loss: 0.1504 | T: 225.13s
| It: 40001 | Rec. loss: 7.6564 |  Task loss: 0.1649 | T: 229.78s
| It: 41001 | Rec. loss: 7.6273 |  Task loss: 0.1620 | T: 227.57s
| It: 42001 | Rec. loss: 7.5402 |  Task loss: 0.1545 | T: 233.04s
| It: 43001 | Rec. loss: 7.5304 |  Task loss: 0.1513 | T: 230.29s
| It: 44001 | Rec. loss: 7.4976 |  Task loss: 0.1564 | T: 225.74s
| It: 45001 | Rec. loss: 7.4345 |  Task loss: 0.1597 | T: 224.35s
| It: 46001 | Rec. loss: 7.3716 |  Task loss: 0.1435 | T: 237.84s
| It: 47001 | Rec. loss: 7.2982 |  Task loss: 0.1478 | T: 228.31s
| It: 48001 | Rec. loss: 7.2407 |  Task loss: 0.1456 | T: 227.17s
| It: 49001 | Rec. loss: 7.1912 |  Task loss: 0.1420 | T: 239.69s
| It: 50001 | Rec. loss: 7.1149 |  Task loss: 0.1428 | T: 240.12s
| It: 51001 | Rec. loss: 7.0686 |  Task loss: 0.1431 | T: 232.01s
| It: 52001 | Rec. loss: 7.0456 |  Task loss: 0.1449 | T: 222.64s
| It: 53001 | Rec. loss: 6.9241 |  Task loss: 0.1467 | T: 234.21s
| It: 54001 | Rec. loss: 6.8598 |  Task loss: 0.1424 | T: 239.66s
| It: 55001 | Rec. loss: 6.7935 |  Task loss: 0.1389 | T: 230.35s
| It: 56001 | Rec. loss: 6.7490 |  Task loss: 0.1421 | T: 227.70s
| It: 57001 | Rec. loss: 6.6991 |  Task loss: 0.1397 | T: 227.06s
| It: 58001 | Rec. loss: 6.5974 |  Task loss: 0.1375 | T: 231.04s
| It: 59001 | Rec. loss: 6.5244 |  Task loss: 0.1495 | T: 228.61s
| It: 60001 | Rec. loss: 6.4458 |  Task loss: 0.1410 | T: 229.88s
| It: 61001 | Rec. loss: 6.3633 |  Task loss: 0.1383 | T: 230.77s
| It: 62001 | Rec. loss: 6.2987 |  Task loss: 0.1441 | T: 230.83s
| It: 63001 | Rec. loss: 6.2440 |  Task loss: 0.1473 | T: 222.92s
| It: 64001 | Rec. loss: 6.1501 |  Task loss: 0.1437 | T: 222.84s
| It: 65001 | Rec. loss: 6.0398 |  Task loss: 0.1411 | T: 237.10s
| It: 66001 | Rec. loss: 5.9983 |  Task loss: 0.1388 | T: 250.58s
| It: 67001 | Rec. loss: 5.8817 |  Task loss: 0.1423 | T: 267.12s
| It: 68001 | Rec. loss: 5.8138 |  Task loss: 0.1284 | T: 273.28s
| It: 69001 | Rec. loss: 5.7223 |  Task loss: 0.1309 | T: 259.11s
| It: 70001 | Rec. loss: 5.6167 |  Task loss: 0.1268 | T: 259.68s
| It: 71001 | Rec. loss: 5.5333 |  Task loss: 0.1338 | T: 261.19s
| It: 72001 | Rec. loss: 5.4712 |  Task loss: 0.1313 | T: 257.96s
| It: 73001 | Rec. loss: 5.3573 |  Task loss: 0.1250 | T: 271.35s
| It: 74001 | Rec. loss: 5.2469 |  Task loss: 0.1258 | T: 273.23s
| It: 75001 | Rec. loss: 5.1674 |  Task loss: 0.1314 | T: 270.88s
| It: 76001 | Rec. loss: 5.0777 |  Task loss: 0.1261 | T: 277.76s
| It: 77001 | Rec. loss: 4.9581 |  Task loss: 0.1294 | T: 256.39s
| It: 78001 | Rec. loss: 4.8730 |  Task loss: 0.1210 | T: 258.05s
| It: 79001 | Rec. loss: 4.7755 |  Task loss: 0.1233 | T: 255.43s
| It: 80001 | Rec. loss: 4.6800 |  Task loss: 0.1247 | T: 258.12s
| It: 81001 | Rec. loss: 4.5742 |  Task loss: 0.1214 | T: 252.92s
| It: 82001 | Rec. loss: 4.4566 |  Task loss: 0.1209 | T: 233.84s
| It: 83001 | Rec. loss: 4.3993 |  Task loss: 0.1233 | T: 239.45s
| It: 84001 | Rec. loss: 4.3040 |  Task loss: 0.1250 | T: 234.73s
| It: 85001 | Rec. loss: 4.2014 |  Task loss: 0.1153 | T: 238.03s
| It: 86001 | Rec. loss: 4.0790 |  Task loss: 0.1151 | T: 230.10s
| It: 87001 | Rec. loss: 3.9922 |  Task loss: 0.1184 | T: 222.81s
| It: 88001 | Rec. loss: 3.9032 |  Task loss: 0.1172 | T: 237.28s
| It: 89001 | Rec. loss: 3.7805 |  Task loss: 0.1213 | T: 234.11s
| It: 90001 | Rec. loss: 3.7040 |  Task loss: 0.1151 | T: 236.59s
| It: 91001 | Rec. loss: 3.6202 |  Task loss: 0.1128 | T: 238.08s
| It: 92001 | Rec. loss: 3.5099 |  Task loss: 0.1112 | T: 227.26s
| It: 93001 | Rec. loss: 3.4303 |  Task loss: 0.1100 | T: 231.61s
| It: 94001 | Rec. loss: 3.3481 |  Task loss: 0.1086 | T: 236.43s
| It: 95001 | Rec. loss: 3.2440 |  Task loss: 0.1101 | T: 238.73s
| It: 96001 | Rec. loss: 3.1700 |  Task loss: 0.1097 | T: 242.02s
| It: 97001 | Rec. loss: 3.0845 |  Task loss: 0.1065 | T: 226.70s
| It: 98001 | Rec. loss: 3.0053 |  Task loss: 0.1078 | T: 227.92s
| It: 99001 | Rec. loss: 2.9296 |  Task loss: 0.1069 | T: 242.63s
| It: 100001 | Rec. loss: 2.8583 |  Task loss: 0.1030 | T: 239.19s
| It: 101001 | Rec. loss: 2.7723 |  Task loss: 0.1007 | T: 229.87s
| It: 102001 | Rec. loss: 2.6941 |  Task loss: 0.1026 | T: 233.76s
| It: 103001 | Rec. loss: 2.6230 |  Task loss: 0.1029 | T: 231.32s
| It: 104001 | Rec. loss: 2.5497 |  Task loss: 0.0982 | T: 226.52s
| It: 105001 | Rec. loss: 2.5067 |  Task loss: 0.1004 | T: 238.15s
| It: 106001 | Rec. loss: 2.4151 |  Task loss: 0.0998 | T: 230.78s
| It: 107001 | Rec. loss: 2.3554 |  Task loss: 0.0996 | T: 233.76s
| It: 108001 | Rec. loss: 2.2843 |  Task loss: 0.0997 | T: 235.55s
| It: 109001 | Rec. loss: 2.2219 |  Task loss: 0.0967 | T: 226.50s
| It: 110001 | Rec. loss: 2.1664 |  Task loss: 0.0953 | T: 227.59s
| It: 111001 | Rec. loss: 2.0893 |  Task loss: 0.0933 | T: 230.25s
| It: 112001 | Rec. loss: 2.0390 |  Task loss: 0.0921 | T: 226.80s
| It: 113001 | Rec. loss: 1.9744 |  Task loss: 0.0924 | T: 220.44s
| It: 114001 | Rec. loss: 1.9196 |  Task loss: 0.0910 | T: 226.18s
| It: 115001 | Rec. loss: 1.8612 |  Task loss: 0.0910 | T: 236.44s
| It: 116001 | Rec. loss: 1.8079 |  Task loss: 0.0914 | T: 236.13s
| It: 117001 | Rec. loss: 1.7803 |  Task loss: 0.0887 | T: 231.45s
| It: 118001 | Rec. loss: 1.7141 |  Task loss: 0.0895 | T: 225.87s
| It: 119001 | Rec. loss: 1.6581 |  Task loss: 0.0893 | T: 222.56s
| It: 120001 | Rec. loss: 1.6097 |  Task loss: 0.0896 | T: 229.27s
| It: 121001 | Rec. loss: 1.5593 |  Task loss: 0.0895 | T: 233.07s
| It: 122001 | Rec. loss: 1.5226 |  Task loss: 0.0872 | T: 240.54s
| It: 123001 | Rec. loss: 1.4703 |  Task loss: 0.0880 | T: 238.08s
| It: 124001 | Rec. loss: 1.4237 |  Task loss: 0.0849 | T: 224.97s
| It: 125001 | Rec. loss: 1.3799 |  Task loss: 0.0840 | T: 233.99s
| It: 126001 | Rec. loss: 1.3373 |  Task loss: 0.0813 | T: 233.34s
| It: 127001 | Rec. loss: 1.3077 |  Task loss: 0.0808 | T: 234.22s
| It: 128001 | Rec. loss: 1.2579 |  Task loss: 0.0810 | T: 238.35s
| It: 129001 | Rec. loss: 1.2175 |  Task loss: 0.0811 | T: 241.42s
| It: 130001 | Rec. loss: 1.1847 |  Task loss: 0.0796 | T: 237.45s
| It: 131001 | Rec. loss: 1.1433 |  Task loss: 0.0793 | T: 233.84s
| It: 132001 | Rec. loss: 1.1060 |  Task loss: 0.0789 | T: 238.46s
| It: 133001 | Rec. loss: 1.0725 |  Task loss: 0.0784 | T: 234.09s
| It: 134001 | Rec. loss: 1.0417 |  Task loss: 0.0777 | T: 238.85s
| It: 135001 | Rec. loss: 1.0066 |  Task loss: 0.0786 | T: 242.42s
| It: 136001 | Rec. loss: 0.9758 |  Task loss: 0.0772 | T: 244.46s
| It: 137001 | Rec. loss: 0.9464 |  Task loss: 0.0769 | T: 240.34s
| It: 138001 | Rec. loss: 0.9193 |  Task loss: 0.0767 | T: 256.26s
| It: 139001 | Rec. loss: 0.8938 |  Task loss: 0.0773 | T: 247.02s
| It: 140001 | Rec. loss: 0.8677 |  Task loss: 0.0769 | T: 241.23s
| It: 141001 | Rec. loss: 0.8434 |  Task loss: 0.0761 | T: 240.26s
| It: 142001 | Rec. loss: 0.8219 |  Task loss: 0.0763 | T: 237.71s
| It: 143001 | Rec. loss: 0.7984 |  Task loss: 0.0765 | T: 232.87s
| It: 144001 | Rec. loss: 0.7808 |  Task loss: 0.0767 | T: 230.32s
| It: 145001 | Rec. loss: 0.7613 |  Task loss: 0.0766 | T: 231.30s
| It: 146001 | Rec. loss: 0.7445 |  Task loss: 0.0763 | T: 236.92s
| It: 147001 | Rec. loss: 0.7293 |  Task loss: 0.0760 | T: 235.12s
| It: 148001 | Rec. loss: 0.7052 |  Task loss: 0.0761 | T: 226.56s
| It: 149001 | Rec. loss: 0.6659 |  Task loss: 0.0762 | T: 229.63s
| It: 150001 | Rec. loss: 0.6420 |  Task loss: 0.0762 | T: 226.28s
| It: 151001 | Rec. loss: 0.6276 |  Task loss: 0.0762 | T: 226.60s
| It: 152001 | Rec. loss: 0.6165 |  Task loss: 0.0761 | T: 230.10s
| It: 153001 | Rec. loss: 0.6056 |  Task loss: 0.0761 | T: 229.79s
| It: 154001 | Rec. loss: 0.5972 |  Task loss: 0.0759 | T: 235.75s
| It: 155001 | Rec. loss: 0.5903 |  Task loss: 0.0760 | T: 234.43s
| It: 156001 | Rec. loss: 0.5848 |  Task loss: 0.0759 | T: 249.29s
| It: 157001 | Rec. loss: 0.5806 |  Task loss: 0.0760 | T: 240.73s
| It: 158001 | Rec. loss: 0.5777 |  Task loss: 0.0759 | T: 225.67s
| It: 159001 | Rec. loss: 0.5763 |  Task loss: 0.0759 | T: 229.97s
| It: 160000 | Rec. loss: 0.5760 |  Task loss: 0.0759 | T: 232.99s
Optimal candidate solution with rec. loss 1671.3074 selected.
Reconstruction stats:
Starting evaluations for attack effectiveness report...
/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
The size of tensor a (55) must match the size of tensor b (61) at non-singleton dimension 3
None
Saved to /user/gparrella/breaching/my_test/optimization_based/grad_inversion/results/vggface2_flickr_8_post2_1.png
========================================================================

